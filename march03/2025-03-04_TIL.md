# Today - 2025-03-04 (Tue)

## Scrub
- 모델 아키텍처 개념 정리 및 학습
- 다양한 신경망 구조와 전이 학습 기법 탐색
- ResNet, VGG16, Transfer Learning에 대한 이해 심화

## Learned

### Model Architecture
- 모델 아키텍처는 신경망의 계층 구조, 연결 방식, 활성화 함수 및 입출력 형태를 포함한 전체적인 구조를 의미함
- 딥러닝에서 모델 아키텍처는 학습을 통해 최적의 가중치를 찾아가는 과정
- 대표적인 모델 아키텍처: CNN, RNN, Transformer 등
- 모델 파일에는 학습된 **가중치(weight)와 편향(bias)**만 저장됨
- 모델 구조 자체는 코드나 별도의 메타데이터로 관리

### 딥러닝 학습 과정
1. **Feed-Forward (순전파)**: 입력 데이터가 각 레이어를 거쳐 변환됨
2. **Loss Function (손실 함수)**: 예측값과 실제값의 차이를 계산하여 손실을 평가
3. **Backpropagation (역전파)**: 손실을 줄이기 위해 가중치를 조정
4. **Optimization (최적화)**: 가중치를 최적화 알고리즘으로 업데이트
5. **반복(Epochs)**: 위 과정을 반복하여 학습 진행

### Pre-trained Model, 사전 학습 모델
- 사전 학습된 모델을 사용하면 모델을 처음부터 학습하는 것보다 **시간과 자원을 절약** 가능
- 모델의 분류기(Classification Layer) 부분만 조정하는 방식으로 활용 가능
- TensorFlow와 PyTorch 모두 사전 학습 모델 지원

### Residual Network (ResNet)
- **잔차(Residual)를 학습하는 구조**
- 신경망이 깊어질수록 발생하는 **Vanishing Gradient 문제를 해결**하기 위해 등장
- **Skip Connection(잔차 연결)**을 통해 이전 층의 정보를 활용하여 학습 안정성을 높임

```python
class CustomResNet50(nn.Module):
    def __init__(self, num_classes):
        super(CustomResNet50, self).__init__()
        self.base_model = resnet50(pretrained=True)
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc1 = nn.Linear(2048, 256)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(256, num_classes)
        self.softmax = nn.Softmax(dim=1)
    
    def forward(self, x):
        x = self.base_model(x)
        x = self.global_avg_pool(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x
```

### VGG16 (Visual Geometry Group)
- CNN 기반 아키텍처로 **13개의 합성곱 계층 + 2개의 완전 연결층(FC Layer)**로 구성
- **Max Pooling**을 활용하여 점진적으로 해상도를 줄이면서 특징을 추출
- 전이 학습(Transfer Learning)에서 뛰어난 성능을 보임

### Transfer Learning (전이 학습)
- 기존 모델을 활용하여 새로운 문제에 적용하는 기법
- 다양한 방식으로 활용 가능:
  1. **Frozen Model**: 사전 학습된 모델을 그대로 사용
  2. **Feature Extraction**: 신경망의 하위 계층은 고정하고, 최상위 계층만 학습
  3. **Partial Fine-Tuning**: 특정 레이어만 학습
  4. **Full Fine-Tuning**: 모든 레이어를 학습

### Overfitting (과적합) vs Underfitting (과소적합)
- **과적합(Overfitting)**: 학습 데이터에 너무 최적화되어 새로운 데이터에서 성능이 저하됨
  - 해결 방법: Dropout, 데이터 증강, 정규화, 조기 종료(Early Stopping), 앙상블 기법 등
- **과소적합(Underfitting)**: 모델이 데이터의 패턴을 제대로 학습하지 못함
  - 해결 방법: 더 깊은 네트워크 사용, 비선형 활성화 함수 적용, 학습 데이터 추가

## Keep
- ResNet, VGG16을 활용한 전이 학습 실습 진행
- EfficientNetB0을 사용하여 PyTorch 예제 포팅
- 사전 훈련된 모델과 직접 학습한 모델의 성능 비교 진행

## Problem
- 전이 학습에서 Feature Extraction과 Fine-Tuning을 혼합하는 최적의 방법을 찾는 과정 필요
- 모델 성능 비교를 위한 데이터셋 설정이 중요

## Try
- VGG16을 활용한 Feature Extraction 및 Partial Fine-Tuning 진행
- 학습률 튜닝과 최적화 알고리즘 테스트
- 같은 모델에 대해 사전 학습된 것과 직접 학습한 것의 성능을 비교

## 참고 자료
- [ResNet 논문](https://arxiv.org/abs/1512.03385)
- [VGG 논문](https://arxiv.org/abs/1409.1556)
- [Transfer Learning 개념 정리](https://www.tensorflow.org/tutorials/images/transfer_learning)

