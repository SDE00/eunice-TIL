# Today - 2025-04-24 (Thu)

## Scrub
- GCP에서 CUDA가 모델 요청을 어떻게 순차 처리하는지 내부 메커니즘 학습
- LangSmith와 LangChain에 대한 아키텍처 이해 및 도입 필요성 검토
- STT + LLM 기반 피드백 시스템 설계에 이러한 기술들이 어떤 의미를 가지는지 분석

## Learned

### GCP의 구조와 CUDA 처리
- GCP는 프로젝트 단위로 VM, DB, AI 모델 등을 리소스로 관리하는 클라우드 플랫폼
- GPU는 CUDA 큐를 통해 **여러 모델 요청을 순차적으로 처리**함
  - 동일 GPU에 3개 모델이 올라갈 경우, CUDA 큐에서 하나씩 순차적으로 처리됨
  - 이 구조에서는 긴 모델이 먼저 점유하면 후속 요청이 지연되므로 **처리 순서와 분리 배치 전략**이 중요

### CUDA 큐 처리 구조 예시
```plaintext
[서버 A - L4 GPU]
 ├── 모델 A (Whisper STT)
 ├── 모델 B (LLM 요약)
 └── 모델 C (면접 피드백)

요청 흐름:
1. Whisper가 음성 처리로 먼저 자원 점유
2. STT 완료 후, LLM이 피드백 생성 수행
3. 마지막으로 요약 모델이 동작 → 모두 순차 처리
```

> 병렬 처리가 아닌 큐 기반 처리이므로, **실시간성이 필요한 모델은 분리 배포 고려 필수**

### LangSmith 개념 및 사용 목적
- LangChain 팀이 개발한 LLM 앱 추적 및 품질 평가 도구

| 기능 | 설명 |
|------|------|
| 세션 추적 | Prompt → LLM → 응답 흐름 기록 |
| 실시간 로그 | 처리 시간, 오류 발생 위치 확인 가능 |
| 에러 추적 | 프롬프트 실패, Timeout 등 시각화 가능 |
| 평가 통합 | 기준 응답과의 유사도 자동 비교 |

→ Prompt 품질, 모델 latency 분석, 에러 구간 추적까지 가능한 **운영 대시보드 기반 품질 모니터링 툴**

### LangChain 도입 필요성
- LangChain은 프롬프트 체이닝, 외부 도구 연동, 세션 관리 등을 위한 LLM 프레임워크

| 적용 필요 여부 | 설명 |
|----------------|------|
| STT → 요약 → 피드백 흐름 |  체이닝 구조 필요 |
| REST 기반 단순 호출 | LangChain 없이도 가능 |
| 세션 기억 / 조건 분기 처리 | LangChain의 memory 및 flow control 기능 활용 가능 |

> 따라서, **현재 기획 중인 STT + LLM 기반 면접 피드백 생성 흐름**에 LangChain은 유의미한 역할을 할 수 있음

## Keep
- GCP에서의 CUDA 큐 처리 흐름과 단일 GPU에서의 자원 분배 정책을 명확히 이해함
- LangSmith는 성능 모니터링뿐 아니라, **분리 배포 기준 판단의 지표**로 활용 가능

## Problem
- 아직 LangChain을 실제 프로젝트에 어떻게 도입할지에 대한 설계 문서 작성은 미비
- LangSmith 연동 실습 및 평가 자동화 구현은 향후 진행 예정

## Try
- 내일 목표: LangChain 도입 필요성 및 실제 적용 시나리오를 중심으로 아키텍처 설계 문서 작성
- LangChain 기반 체이닝 예제 및 각 체인 단위의 평가 지표 정의

## 참고자료
- [CUDA Programming Guide - NVIDIA](https://docs.nvidia.com/cuda/)
- [LangSmith 공식 문서](https://docs.smith.langchain.com/)
- [LangChain 공식 사이트](https://www.langchain.com/)
- [LangChain + LangSmith 연동 예시](https://github.com/langchain-ai/langsmith)
- [GCP GPU 할당 정책](https://cloud.google.com/compute/docs/gpus)