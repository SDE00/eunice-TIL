# Today - 2025-04-23 (Wed)

## Scrub
- STT 성능 비교 실험을 위해 Whisper / GCP STT / AWS Transcribe 세 가지 모델을 선정하고 테스트 실행
- Whisper 모델 내부에서도 base 버전을 선택하여 실험 반복성과 메모리 안정성을 확보
- 모델별 특징, 운영 적합성, 비용 대비 성능 비교 관점을 기준으로 실험 계획 수립

---

## Learned

### STT 모델 비교 실험 대상 선정 이유

| 모델 | 선택 이유 | 비교 목적 |
|------|-----------|-----------|
| Whisper (OpenAI) | - 오픈소스이자 무료<br>- Colab에서 직접 실행 가능<br>- 최근 연구에서 높은 정확도<br>- 한국어 지원 강력 | 현실적인 비용 대비 성능 기준 |
| GCP STT | - Google 대규모 데이터 기반<br>- REST & Streaming API 지원<br>- 다국어 및 커스텀 모델 지원 | 상용 STT 중 신뢰도 높은 기준 |
| AWS Transcribe | - 실제 서비스에서 가장 많이 사용<br>- 자동 문장 구분, 시간정보 제공<br>- S3 연동 등 유연한 운영 | 클라우드 기반 운영 환경 기준 |

> 비교 목적: Whisper (로컬 기반) vs GCP, AWS (클라우드 상용 API)의 비용/성능 균형 측정  
> 시스템 아키텍처와도 부합: GCP 기반으로 기획된 현재 프로젝트 구조와 연계 가능

---

### Whisper 내부 버전 선택 기준

| 버전 | 파라미터 수 | 장점 | 단점 |
|------|--------------|------|------|
| tiny | 39M | 빠름, 자원 소모 적음 | 정확도 낮음 |
| base | 74M | 속도/정확도 균형 우수, Colab에서 안정적 실행 | 대용량 대비 다소 낮은 정확도 |
| small | 244M | 정확도 향상, 여전히 경량 | 처리 시간 증가 |
| medium | 769M | 정확도 매우 높음 | Colab에서 느리고 메모리 부족 가능성 |
| large-v2 | 1550M | 최고 정확도, 논문 기준 SOTA | Colab 실행 어려움, 느림 |

> 최종 선택: **Whisper base**

**선정 이유**
- 20분 분량 음성도 안정적으로 처리 가능
- Colab Pro (T4) 환경에서도 메모리 초과 없이 실행 가능
- small과 유사한 정확도, tiny보다 빠른 속도
- 반복 실험 가능성과 성능 측정의 타당성 확보

---

## Keep
- 실험 설계 시 Colab 환경 제약 고려하여 모델 크기 및 실행 가능성 검토
- 오픈소스(STT)와 상용 API(GCP/AWS)의 비교 기준을 명확히 정립한 점

## Problem
- AWS Transcribe 결과의 문장 분리 정확도 확인 필요
- STT 응답 포맷이 모델별로 상이하여 후처리 통일 필요

## Try
- 다음 단계로 20분 분량 데이터를 입력하여 LLM 기반 피드백 생성 성능 평가 예정
- Whisper 외 STT 모델들의 비용 대비 실시간 처리 가능성 검토 예정