# Today - 2025-04-11 (Fri)

## Scrub
- 대표자 미팅을 통해 **모델 선정 및 성능 평가 기준 설정**에 대한 논의 진행
- 단순 모델 실험을 넘어서, **질문 생성-평가-개선 흐름 전체를 구조화해야 한다는 필요성** 인식

## Learned

### 모델 실험 및 선정 계획
- 다음 주 화요일(4/15)까지 모델 비교 실험 후 최종 선정 예정
- 후보 모델: **DeepSeek**, **LLaMA**, **Gemma**
- 모델 실험은 단순 정확도가 아닌, 생성 질문의 품질·도메인 적합성·자연스러움 기준으로 평가

### 제출 예정 항목 (PL 미팅 전)
| 항목 | 설명 | 마감 기한 |
|------|------|------------|
| API 설계 | 요청-응답 구조 및 흐름 정리 | 2025-04-15 |
| 아키텍처 | 전체 시스템 구성도, 모델 위치 포함 | 2025-04-15 |
| 최적화 | 추론 속도, 자원 분배, 로깅 등 운영 전략 포함 | 2025-04-15 |

### 성능 평가 지표 (BMT 초안)
- 실제 면접 질문으로 적절한가?
- 직무 및 산업 도메인에 맞는 질문인가?
- 꼬리 질문이 자연스럽고 깊이 있게 연결되는가?
- 생성 질문이 중복되거나 지나치게 단순하지 않은가?

> 위 기준은 향후 파인튜닝 모델의 성능 향상 정도를 **정량적 수치로 비교**하기 위한 기반으로 활용 예정

### 시스템 흐름 구상
- 단지 질문을 생성하는 것이 아니라, 다음과 같은 **전 과정 시각화 및 추적 시스템** 필요:
  1. 입력 정보: 사용자의 이력 / 프로젝트 정보 / 희망 직무
  2. 질문 생성: LLM 기반 질문 생성 모듈
  3. 평가: 품질 평가 기준 적용 (자동/수동 혼합)
  4. 개선 로그: 미흡했던 질문 유형 분류 및 피드백 루프

## Keep
- 단순 실험 → 시스템화로 확장하는 관점이 정립됨

## Problem
- 모델별 질문 샘플 수집 및 비교가 아직 정량화되지 않음 
- 평가 기준에 대한 실질 테스트 및 정합성 검증이 필요

## Try
- 팀원별로 하나씩 모델을 맡아서 테스트 진행 (난 Gemma)
- 평가 기준에 따른 **질문 예시별 등급 매핑 시도**