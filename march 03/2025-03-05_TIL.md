# Today - 2025-03-05 (Wed)

## Scrub
- Early Stopping 개념 학습 및 구현 방법 탐색
- Regularization과 Hyperparameter Tuning 이해 심화
- GridSearch 및 RandomSearch 개념 정리

## Learned
### Early Stopping, 조기 중단
- 학습 데이터에 너무 최적화되어 일반화 성능이 떨어지는 상황(과적합)에서 학습을 중단하는 기법
- **조기 중단 예시**
  - **No Early Stopping**: 학습이 10 에포크까지 진행되었을 때 검증 손실(Val Loss)이 초반에는 감소하지만 6 에포크 이후 다시 증가하며 과적합 발생
  - **Early Stopping at Epoch 4**: 4번째 에포크에서 조기 중단이 발생하여 검증 손실이 증가하기 전에 학습이 멈춤 → 과적합 방지 가능

### Regularization, 정규화
- 과적합을 방지하고 모델의 일반화 성능 향상을 위한 기법 (**특화 ↓ / 일반화 ↑**)
- 모델의 가중치나 뉴런의 활성화를 제어하여 데이터의 변동성, 복잡성에 대한 과도한 적응을 방지
- **정규화 적용 효과**
  - 과적합 방지
  - 훈련 시간 절약
  - 최적의 모델 선택 가능
- **조기 중단 조건 설정 필요**
  - 검증 손실(validation loss)이 더 이상 감소하지 않을 때
  - 검증 정확도(validation accuracy)가 더 이상 증가하지 않을 때
  - 기타 사용자 정의 조건 (예: 검증 손실 증가율이 일정 수준 이하로 떨어지는 시점)

### Hyperparameter Tuning
- 기계 학습 모델의 성능을 최적화하기 위해 모델의 파라미터 값을 조정하는 과정
- CNN의 필터 가중치도 하이퍼파라미터처럼 조정 가능하며, 이를 통해 AI 성능을 향상 가능
- **주요 하이퍼파라미터 목록**
  - 학습 속도 (Learning Rate): 0.01, 0.001, 0.0001
  - 배치 크기 (Batch Size): 32, 64, 128
  - 에포크 (Epochs): 10, 50, 100
  - 옵티마이저 (Optimizer): SGD, Adam, RMSprop
  - 모멘텀 (Momentum): 0.9, 0.95, 0.99
  - 드롭아웃 비율 (Dropout Rate): 0.2, 0.5
  - 정규화 파라미터 (Regularization Parameter): L2 정규화(lambda) 값: 0.01, 0.001
  - 학습률 감쇠 (Learning Rate Decay): 0.1, 0.01, 0.001
  - 활성화 함수 (Activation Function): ReLU, Sigmoid, Tanh
  - 초기화 방법 (Initialization Method): He, Glorot, Random

## Keep
- Early Stopping과 Regularization 기법 비교 실험 진행
- Hyperparameter Tuning의 GridSearch 및 RandomSearch 비교 실험
- Fine-Tuning과 Feature Extraction의 효과 분석

## Problem
- Pytorch에서는 Early Stopping을 기본적으로 제공하지 않음 → TensorFlow에서 구현 필요
- Hyperparameter Tuning 시 학습 속도와 배치 크기의 최적 조합 찾는 과정 필요

## Try
- TensorFlow의 예제 코드를 Pytorch로 변경해 구현해보기