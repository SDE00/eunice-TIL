# Today - 2025-05-27 (Tue)

## Scrub
- Google의 AI Breakfast를 시청하며 "좋은 AI란 무엇인가"에 대해 고민함
- Google DeepMind와 AGI(Artificial General Intelligence)의 철학과 방향성에 주목
- AI 할루시네이션, 그라운딩, RAG 등 실제 AI 신뢰성 확보 방식 학습
- AI의 창의적 활용과 한계에 대한 자기 성찰적 고찰도 병행

## Learned

### 좋은 AI란 무엇인가? – Google의 시각
- 목표: 특정 작업만 잘하는 AI가 아니라 **범용적 AGI**를 추구 -> gemini를 통해 이뤄가는 중..!

#### 접근 방식: **학고한 AI engine을 통한 발전 방향**
> 자체 사용자 테스트 → B2B 확장 → 오픈소스 제공 → Google Cloud 플랫폼화

| 요소 | 설명 |
|------|------|
| 양질의 데이터 | 유튜브 영상 → Veo 등으로 활용 |
| 리소스 자산 | Google은 막대한 GPU 인프라 보유 |
| 안전성 중심 철학 | AI가 생성하는 정보의 사실성(factuality) 우선 |

> Google은 AI 발전을 위해 "데이터 품질"과 "출력 신뢰성"을 중시

---

### AI의 할루시네이션(Hallucination)
- 인간의 뇌가 물 섭취 후 갈증 해결을 "즉시 느끼는 것"과 유사
- AI도 "정답을 모르면서 지어내는" 특성이 있음
- 이는 인간의 **예측 메커니즘**과 유사함

→ AI를 인간처럼 설계했기에 발생하는 문제일 수 있음

---

### Grounding과 RAG의 역할
- **지어내기 방지 = Grounding**
- **RAG (Retrieval-Augmented Generation)**: 실시간 외부 정보로 보강

![Hallucination Leaderboard](https://github.com/vectara/hallucination-leaderboard/raw/main/img/hallucination_rates_with_logo.png)

- 출처: https://github.com/vectara/hallucination-leaderboard
- 정확성과 책임성을 함께 강조: **Responsible + Factuality**

---

### 대표 사례: Notebook LM, Google Workspace
- 논문 요약, 정보 비교, 실시간 개인화된 정보 제공
- "기존 지식 대비 어떤 점이 새로운가?" → 분석 자동화

---

## Keep
- AI의 할루시네이션은 인간 두뇌의 구조적 유사성에서 기인할 수 있다는 점이 흥미로웠음
- Google이 AI를 단계적으로 오픈하며 안전성과 확장성을 동시에 확보하는 방식이 인상적

## Problem
- AI가 ‘지어낸다’는 현상에 대해 단순 오류로 보는 게 아니라, **인지적 패턴**으로 봐야 함
- 결국 "좋은 AI"를 만들려면 → **좋고 나쁨을 판단할 수 있는 기준이 먼저 필요**

## Try
- 향후 LLM 실험 시 **grounding 여부 및 citation 삽입** 체크
- 논문 기반의 AI 응답 평가 실험을 기획해볼 것 (Notebook LM 구조 참고)

## 참고자료
- [Google AI Breakfast 영상](https://www.youtube.com/watch?v=KlojGihJfMU&t=18s)
- [AGI 위키피디아 개념 정리](https://en.wikipedia.org/wiki/Artificial_general_intelligence)
- [할루시네이션 비교 리더보드](https://github.com/vectara/hallucination-leaderboard)
- [AI 창의적 활용 사례](https://www.youtube.com/watch?v=rSS5yM74zeo&t=345s)